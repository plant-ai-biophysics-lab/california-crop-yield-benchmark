{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataset_wrapper as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "                .geemap-dark {\n",
       "                    --jp-widgets-color: white;\n",
       "                    --jp-widgets-label-color: white;\n",
       "                    --jp-ui-font-color1: white;\n",
       "                    --jp-layout-color2: #454545;\n",
       "                    background-color: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-dark .jupyter-button {\n",
       "                    --jp-layout-color3: #383838;\n",
       "                }\n",
       "\n",
       "                .geemap-colab {\n",
       "                    background-color: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "\n",
       "                .geemap-colab .jupyter-button {\n",
       "                    --jp-layout-color3: var(--colab-primary-surface-color, white);\n",
       "                }\n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2008/yield_2008.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2009/yield_2009.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2010/yield_2010.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2011/yield_2011.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2013/yield_2013.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2014/yield_2014.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2015/yield_2015.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2016/yield_2016.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2017/yield_2017.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2018/yield_2018.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2019/yield_2019.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2020/yield_2020.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2021/yield_2021.csv\n",
      "Processed and saved: /data2/hkaman/Data/FoundationModel/Inputs/Sonoma/InD/2022/yield_2022.csv\n"
     ]
    }
   ],
   "source": [
    "df = util.ProcessingYieldObs(\n",
    "    county_name = \"Sonoma\", \n",
    "    output_root_dir= '/data2/hkaman/Data/FoundationModel/Inputs'\n",
    ")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate yearly dataframe after cleaning crop names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_county_crop_dataset.ccdc import cdl_crop_legend, ProcessYieldObs\n",
    "\n",
    "df = util.ProcessingYieldObs(\n",
    "    county_name = \"Monterey\"\n",
    ")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./all.csv')\n",
    "# crop_name_mapping = df.groupby(\"crop_name\")[\"key_crop_name\"].unique().to_dict()\n",
    "# crop_name_mapping = {k: list(v) for k, v in crop_name_mapping.items()}\n",
    "# mapping_df = pd.DataFrame(crop_name_mapping.items(), columns=[\"crop_name\", \"key_crop_name\"])\n",
    "# mapping_df.to_csv(\"crop_name_mapping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset County Based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_county_crop_dataset import ccdc\n",
    "import pandas as pd\n",
    "m = pd.read_csv('/data2/hkaman/Data/FoundationModel/Monterey/Yield/2008/yield_2008.csv')\n",
    "crop_names = m['key_crop_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = ccdc.TT(\n",
    "    county_name = 'Monterey', \n",
    "    year = 2008, \n",
    "    crop_names = crop_names[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Soil Attributes in Data: ['aws0100wta', 'slopegraddcp', 'awmmfpwwta', 'drclassdcd', 'hydgrpdcd']\n"
     ]
    }
   ],
   "source": [
    "output_dataset = downloader(output_type = \"soil_data\", daily_climate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 963)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "output_dataset['Herbs']['soil_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 7) (60, 9) (39, 9)\n",
      "(9, 6, 958) (12, 2, 958) (365, 8, 2) (1, 958) 18.31\n"
     ]
    }
   ],
   "source": [
    "dataLoader.dataloader(\n",
    "    county_name='Monterey'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_update_dataframes(base_path, county_name):\n",
    "    \"\"\"\n",
    "    Reads .npz files and their corresponding .csv files, checks if any of the four datasets\n",
    "    have a last dimension of 0, removes those rows from the dataframe, and saves the cleaned file.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): The base directory containing county-specific folders.\n",
    "        county_name (str): The county name to process.\n",
    "    \"\"\"\n",
    "\n",
    "    base_csv_path = os.path.join(base_path, county_name, \"InD\")\n",
    "\n",
    "    # Loop over each year from 2008 to 2022 (except 2012)\n",
    "    for year in range(2008, 2023):\n",
    "        if year == 2012:\n",
    "            continue  # Skip 2012 as it does not exist\n",
    "\n",
    "        folder_path = os.path.join(base_csv_path, str(year))\n",
    "        csv_file = os.path.join(folder_path, f\"yield_{year}.csv\")\n",
    "\n",
    "        npz_file_path = os.path.join(folder_path, f\"{county_name}_{year}.npz\")\n",
    "\n",
    "        # Check if both files exist\n",
    "        if not os.path.exists(csv_file):\n",
    "            print(f\"CSV file not found for {year}: {csv_file}\")\n",
    "            continue\n",
    "        if not os.path.exists(npz_file_path):\n",
    "            print(f\"NPZ file not found for {year}: {npz_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df[df['key_crop_name'] != 'No Match']\n",
    "\n",
    "        # Load the NPZ file\n",
    "        try:\n",
    "            loaded_data = np.load(npz_file_path, allow_pickle=True)[\"inumpyut\"]\n",
    "            loaded_data = loaded_data.item()  # Convert NumPy object to a dictionary\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NPZ file for {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Identify rows to keep\n",
    "        valid_rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            crop_name = row[\"key_crop_name\"]\n",
    "\n",
    "            # Ensure crop exists in the dictionary\n",
    "            if crop_name not in loaded_data:\n",
    "                print(f\"Warning: Crop '{crop_name}' not found in NPZ file for {year}. Skipping row {idx}.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract data\n",
    "                landsat = loaded_data[crop_name]['landsat_data']\n",
    "                et = loaded_data[crop_name]['et_data']\n",
    "                climate = loaded_data[crop_name]['climate_data']\n",
    "                soil = loaded_data[crop_name]['soil_data']\n",
    "\n",
    "                # Check last dimension size\n",
    "                if (\n",
    "                    landsat.shape[-1] > 0 and \n",
    "                    et.shape[-1] > 0 and \n",
    "                    climate.shape[-1] > 0 and \n",
    "                    soil.shape[-1] > 0\n",
    "                ):\n",
    "                    valid_rows.append(idx)  # Keep only valid rows\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {crop_name} in {year}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Filter the dataframe to keep only valid rows\n",
    "        df_cleaned = df.loc[valid_rows].reset_index(drop=True)\n",
    "\n",
    "        # Save the updated dataframe back to the same location\n",
    "        df_cleaned.to_csv(csv_file, index=False)\n",
    "        print(f\"Updated CSV saved for {year}: {csv_file} (Removed {len(df) - len(df_cleaned)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_name = 'Yolo'\n",
    "base_csv_path = f'/data2/hkaman/Data/FoundationModel/Inputs'\n",
    "clean_and_update_dataframes(base_csv_path, county_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
